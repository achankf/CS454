\section{Name Directory And {\tt rpcCacheCall}}
To (mostly) synchronize name directories, each machine contains a local in-memory log, versioned by timestamp ordering.
The log is represented by a {\tt std::vector(LogEntry)}.
Thus, the version of the name directory is simply the size of the vector.
{\tt LogEntry} is one of the following:

\begin{itemize}
\item
\begin{verbatim}
NEW_NODE id ip_addr listen_port
\end{verbatim}
where {\tt id} is an unique incremental id generated by the binder, {\tt ip\_addr} is the ipv4 address of a server machine, and {\tt listen\_port} is the port number that the machine uses to listen incoming messages.
\item
\begin{verbatim}
KILL_NODE id
\end{verbatim}
This entry removes entries that are related to the server machine of {\tt id}.
\item
\begin{verbatim}
NEW_FUNC id func
\end{verbatim}
This entry associates a function definition to the server machine of {\tt id}.
\end{itemize}

Notice that a server always has up-to-date information about itself, because each request that affects the binder's name directory has a reply with the changes attached.
Also, when a server runs {\tt rpcExecute}, it forces the binder to broadcast {\tt NEW\_SERVER\_EXECUTE}, which causes all servers to ask the binder for the latest updates.

\subsection{The Simple \tt rpcCacheCall}
The algorithm is the same as the one described by the specification.
The algorithm goes like this
\begin{algorithm}
let S be an empty set of integer ids
while local name directory has suggestions
	// round-robin, same code as the binder
	let i be the suggestion

	if i is in S then break // reaching a cycle
	otherwise let S = S union {i}

	let server be resolve(i) from the name directory

	start a scoped connection to server
		if server is connected then
			send EXECUTE request
			return if got either OK or SKELETON_FAILURE
		end if
	end connection // happens automatically by RAII

end while loop

// local cache failed, so use the binder as fallback
run rpcCall and return whatever from it
\end{algorithm}

Notice that the a set is used to detect cycles.
That's right, it is possible that there alive servers but they reject the execute request due to having no available worker threads (see optimization, the last section).
To keen readers, you may argue that returning the pivot is more efficient, which I just realize at the time writing.
That's true, since the pivot wraps around the container, so comparing $i$ to the pivot is an easy way to detect a cycle.
But, but, but, {\tt NameService::suggest()} is amortized $\Theta(n)$ (number of candadates)  anyways, to due {\tt std::advance}, and running an extra $\Theta(\log n)$ for {\tt std::set::find} and {\tt std::set:insert} won't hurt anyways, right?
In fact, had I store the iterator instead of a pivot the cost of {\tt suggest} may become amortized $O(1)$.
Well, I am just lazy to the change the codes.

Ok, I admit I am just high, since I haven't used the listing environment since taking my last algorithm course.

\subsection{Cases When Name Directory Is Not Up-to-date}
Recall my design decision that all remote connections are scoped.
This means the binder doesn't know immediately when a server dies (for whatever unnatural reasons).
Thus, {\tt KILL\_NODE} entries are added lazily to the logs when the binder realizes a server is down.
This means some entries actually can be zombies.
The binder knows whether a server live in the following circumstances:
\begin{itemize}
\item
The binder broadcast {\tt NEW\_SERVER\_EXECUTE} to every server.
\item
The client ask for a {\tt LOC\_REQUEST}, which causes the binder to look for a suggestion.
To make a suggestion, the binder needs to probe candadate servers by openning connections.
\item
The binder broadcast {\tt TERMINATE} to every server.
However, there is no point to update the name server because the system is terminating.
\end{itemize}

Though, zombie entries are not problematic, because they only cause {\tt rpcCacheCall} to probe more servers, but not the binder.
If a server tries to register a {\tt Name} that is already a zombie (with the same ip address and port), the binder assigns a new id and replaces all old entries.
For clients, there are 2 cases in a successful probe:
\begin{itemize}
\item
All records related to the server is still up-to-date (i.e.\ not zombies).
In this case, the client proceed with the call.
\item
Records related to the server are no longer up-to-date (i.e.\ zombies).
This {\bf only} happens when a server is dead and it reboots, such that the listen port is the same as before.
There are 2 subcases:
\begin{itemize}
\item
The server re-registers functions before the client tries to call them.
In this case, the server will accept the request and will run the call, which is fine because the specification guarantees same functionality for each unique method across servers.
\item
The server is slow and the client catch on before registration is complete.
In this case, the server reply with the error {\tt FUNCTION\_NOT\_REGISTERED}.
The client moves on to the next candadate.
\end{itemize}
\end{itemize}

In any case, I think it is reasonable to assume that servers stay up with little down time, which makes the logs grow very little in rare occassion. So the size of logs in each message is amortized $O(1)$ -- mostly consist of an integer (zero) that represents the number of log entries and nothing follow after.
